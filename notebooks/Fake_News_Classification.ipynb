{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T01:55:43.175944Z",
     "end_time": "2023-04-26T01:55:43.181247Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install kaggle\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle competitions download -c 'fake-news'\n",
    "# !mv fake-news.zip ../tmp/\n",
    "# !unzip ../tmp/fake-news.zip -d ../tmp/fake-news\n",
    "# !rm ../tmp/fake-news.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T01:55:43.530322Z",
     "end_time": "2023-04-26T01:55:43.534489Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import *\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T01:55:43.719722Z",
     "end_time": "2023-04-26T01:55:43.728114Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_device():\n",
    "    if t.cuda.is_available():\n",
    "        return t.device('cuda')\n",
    "    else:\n",
    "        try:\n",
    "            return t.device('mps')\n",
    "        except:\n",
    "            return t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T01:55:44.111400Z",
     "end_time": "2023-04-26T01:55:45.174353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   id                                              title              author   \n0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus  \\\n1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n\n                                                text  label  \n0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n1  Ever get the feeling your life circles the rou...      0  \n2  Why the Truth Might Get You Fired October 29, ...      1  \n3  Videos 15 Civilians Killed In Single US Airstr...      1  \n4  Print \\nAn Iranian woman has been sentenced to...      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>author</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>Darrell Lucus</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n      <td>Daniel J. Flynn</td>\n      <td>Ever get the feeling your life circles the rou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Why the Truth Might Get You Fired</td>\n      <td>Consortiumnews.com</td>\n      <td>Why the Truth Might Get You Fired October 29, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n      <td>Jessica Purkiss</td>\n      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Iranian woman jailed for fictional unpublished...</td>\n      <td>Howard Portnoy</td>\n      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../tmp/fake-news/\"\n",
    "df = pd.read_csv(f'{data_dir}/train.csv')\n",
    "test = pd.read_csv(f'{data_dir}/test.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T01:55:57.699102Z",
     "end_time": "2023-04-26T01:55:57.722025Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.fillna('')\n",
    "test = test.fillna('')\n",
    "\n",
    "df['total'] = df['title']+' '+df['author']\n",
    "test['total']=test['title']+' '+test['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T01:56:35.799049Z",
     "end_time": "2023-04-26T01:56:35.804182Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:15:20.241970Z",
     "end_time": "2023-04-26T02:15:20.262267Z"
    }
   },
   "outputs": [],
   "source": [
    "#Choosing vocabulary size to be 5000 and copying data to msg for further cleaning\n",
    "voc_size = 5000\n",
    "X_train = X_train.copy()\n",
    "X_val = X_val.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:15:22.948376Z",
     "end_time": "2023-04-26T02:15:22.955266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sdfedorov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading stopwords \n",
    "#Stopwords are the words in any language which does not add much meaning to a sentence.\n",
    "#They can safely be ignored without sacrificing the meaning of the sentence.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:15:23.973193Z",
     "end_time": "2023-04-26T02:15:23.979900Z"
    }
   },
   "outputs": [],
   "source": [
    "#We will be using Stemming here\n",
    "#Stemming map words to their root forms\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def construct_corpus(data):\n",
    "    corpus = []\n",
    "    for i in range(len(data)):\n",
    "        review = re.sub('[^a-zA-Z]',' ', data['total'].iloc[i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T02:18:25.670117Z",
     "end_time": "2023-04-26T02:18:25.672270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:18:26.616423Z",
     "end_time": "2023-04-26T02:18:44.112387Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Union, Iterable\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "corpus = construct_corpus(X_train)\n",
    "corpus_test = construct_corpus(X_test)\n",
    "corpus_val = construct_corpus(X_val)\n",
    "\n",
    "tokens = [tokenizer(doc) for doc in corpus]\n",
    "tokens_val = [tokenizer(doc) for doc in corpus_val]\n",
    "tokens_test = [tokenizer(doc) for doc in corpus_test]\n",
    "voc = build_vocab_from_iterator(tokens + tokens_val, max_tokens=voc_size, specials=[\"<unk>\"])\n",
    "voc.set_default_index(voc[\"<unk>\"])\n",
    "\n",
    "voc_tokens = [t.tensor(voc(token), dtype=t.int64) for token in tokens]\n",
    "voc_tokens_val = [t.tensor(voc(token), dtype=t.int64) for token in tokens_val]\n",
    "voc_tokens_test = [t.tensor(voc(token), dtype=t.int64) for token in tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T01:39:14.518308Z",
     "end_time": "2023-04-26T01:39:14.519892Z"
    }
   },
   "outputs": [],
   "source": [
    "# def one_hot_tokens(voc, tokens):\n",
    "#     voc_tokens = [t.tensor(voc(token), dtype=t.int64) for token in tokens]\n",
    "#     return [F.one_hot(t, num_classes = len(voc)) for t in voc_tokens]\n",
    "# one_hot = one_hot_tokens(voc, tokens)\n",
    "# one_hot_test = one_hot_tokens(voc, tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:19:32.073860Z",
     "end_time": "2023-04-26T02:19:32.425602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/16640 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3661ce3800548d096b6090ed2b7b2b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r1n3j5793rn8qxqk1d1p9cbh0000gn/T/ipykernel_34150/2851111432.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding.append(nn.ConstantPad1d((max_len - len(token), 0), 0)(t.tensor(token, dtype=t.int64)))\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4160 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0a52fdf518b4c4a99b715e77628b509"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "680fe46b241543b1bb5bf4a999b69ecc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_len = 25\n",
    "#\n",
    "# def padding_tensor(one_hot_t):\n",
    "#     embedding = []\n",
    "#     for i in tqdm(range(len(one_hot_t))):\n",
    "#         embedding.append(nn.ConstantPad2d((0, 0, max_len - one_hot_t[i].shape[0], 0), 0)(one_hot_t[i]))\n",
    "#     return t.stack(embedding)\n",
    "\n",
    "def padding_indexes(tokens):\n",
    "    embedding = []\n",
    "    for token in tqdm(tokens):\n",
    "        embedding.append(nn.ConstantPad1d((max_len - len(token), 0), 0)(t.tensor(token, dtype=t.int64)))\n",
    "    return t.stack(embedding)\n",
    "    \n",
    "# embedded_docs = padding_tensor(one_hot)\n",
    "# embedded_docs_test = padding_tensor(one_hot_test)\n",
    "\n",
    "padded_tokens = padding_indexes(voc_tokens)\n",
    "padded_tokens_val = padding_indexes(voc_tokens_val)\n",
    "padded_tokens_test = padding_indexes(voc_tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description + training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:48:34.839500Z",
     "end_time": "2023-04-26T02:48:34.845148Z"
    }
   },
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)#, padding_idx=0)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(hidden_dim, 64)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout_2(x)\n",
    "        x = t.relu(self.dense(x))\n",
    "        x = self.dropout_3(x)\n",
    "        x = t.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T02:40:11.356776Z",
     "end_time": "2023-04-26T02:40:11.361465Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokensDataset(Dataset):\n",
    "    def __init__(self, X, y, device):\n",
    "        self.X = X.to(device)\n",
    "        self.y = t.tensor(y).float().to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = optimal_device()\n",
    "print(f\"Device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T02:40:11.986989Z",
     "end_time": "2023-04-26T02:40:11.990152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T03:11:04.789350Z",
     "end_time": "2023-04-26T03:11:04.809052Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "batch_size = 64\n",
    "train_dataset = TokensDataset(padded_tokens, y_train.values, device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TokensDataset(padded_tokens_val, y_val.values, device)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "model = FakeNewsClassifier(voc_size, 40, 100, 2, 0.3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def accuracy(model, test_dataloader):\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        sum_acc = 0\n",
    "        for batch in test_dataloader:\n",
    "            X, y = batch\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.squeeze()\n",
    "            sum_acc += t.sum((y_pred > 0.5) == y)\n",
    "        return sum_acc / (len(test_dataloader) * test_dataloader.batch_size)\n",
    "\n",
    "def train(model, dataloader, test_dataloader, loss, optimizer, epochs):\n",
    "    print(f\"Staring acc_train: {accuracy(model, dataloader)} acc_val: {accuracy(model, test_dataloader)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        l = 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.squeeze()\n",
    "            l = loss(y_pred, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_acc = accuracy(model, dataloader)\n",
    "        val_acc = accuracy(model, test_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} loss: {l.item()} acc_train: {train_acc} acc_val: {val_acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T03:11:05.088973Z",
     "end_time": "2023-04-26T03:11:05.094293Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring acc_train: 0.50390625 acc_val: 0.48750001192092896\n",
      "Epoch 1 loss: 0.19220766425132751 acc_train: 0.9737379550933838 acc_val: 0.96875\n",
      "Epoch 2 loss: 0.0719328299164772 acc_train: 0.9912259578704834 acc_val: 0.9872596263885498\n",
      "Epoch 3 loss: 0.006107015535235405 acc_train: 0.9958533644676208 acc_val: 0.9908654093742371\n",
      "Epoch 4 loss: 0.009487172588706017 acc_train: 0.9964542984962463 acc_val: 0.9911057949066162\n",
      "Epoch 5 loss: 0.008560552261769772 acc_train: 0.997776448726654 acc_val: 0.9923076629638672\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, loss, optimizer, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T03:11:07.123355Z",
     "end_time": "2023-04-26T03:11:37.300784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "(id                                                    11547\n title     Aussie Muslims Demand ’Safe Spaces’ so Followe...\n author                                           Simon Kent\n text      An Australian Islamic group wants   “safe spac...\n total     Aussie Muslims Demand ’Safe Spaces’ so Followe...\n Name: 11547, dtype: object,\n 0)"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.iloc[123], y_val.iloc[123]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T03:21:12.996050Z",
     "end_time": "2023-04-26T03:21:12.999493Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0062]], device='mps:0', grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(padded_tokens_val[123].unsqueeze(0).to(device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T03:21:26.902750Z",
     "end_time": "2023-04-26T03:21:26.916332Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
